{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb85ce8b",
   "metadata": {},
   "source": [
    "Step 1: Loading Data\n",
    "For this tutorial, we’ll use the dataset of papers published in NIPS conference. The NIPS conference (Neural Information Processing Systems) is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
    "\n",
    "Let’s start by looking at the content of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d155738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(\"./NIPS Papers.zip\", \"r\") as zip_ref:\n",
    "    # Extract the file to a temporary directory\n",
    "    zip_ref.extractall(\"temp\")\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
    "\n",
    "# Print head\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d4221",
   "metadata": {},
   "source": [
    "Step 2: Data Cleaning\n",
    "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad361ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>Simplified Rules and Theoretical Analysis for\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6024</th>\n",
       "      <td>Using hippocampal 'place cells' for\\nnavigatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>From Batch to Transductive Online Learning\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>Information Bottleneck for\\nGaussian Variables...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>A Constructive RBF Network\\nfor Writer Adaptat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             paper_text\n",
       "2389  Simplified Rules and Theoretical Analysis for\\...\n",
       "6024  Using hippocampal 'place cells' for\\nnavigatio...\n",
       "1932  From Batch to Transductive Online Learning\\n\\n...\n",
       "1603  Information Bottleneck for\\nGaussian Variables...\n",
       "282   A Constructive RBF Network\\nfor Writer Adaptat..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the columns\n",
    "papers = papers.drop(columns=['id', 'title', 'abstract', \n",
    "                              'event_type', 'pdf_name', 'year'], axis=1)\n",
    "\n",
    "# sample only 100 papers\n",
    "papers = papers.sample(100)\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf7606",
   "metadata": {},
   "source": [
    "Remove punctuation/lower casing\n",
    "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb7c12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2389    simplified rules and theoretical analysis for\\...\n",
       "6024    using hippocampal 'place cells' for\\nnavigatio...\n",
       "1932    from batch to transductive online learning\\n\\n...\n",
       "1603    information bottleneck for\\ngaussian variables...\n",
       "282     a constructive rbf network\\nfor writer adaptat...\n",
       "Name: paper_text_processed, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers['paper_text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89091671",
   "metadata": {},
   "source": [
    "Tokenize words and further clean-up text\n",
    "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2708ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simplified', 'rules', 'and', 'theoretical', 'analysis', 'for', 'information', 'bottleneck', 'optimization', 'and', 'pca', 'with', 'spiking', 'neurons', 'lars', 'buesing', 'wolfgang', 'maass', 'institute', 'for', 'theoretical', 'computer', 'science', 'graz', 'university', 'of', 'technology', 'graz', 'austria', 'larsmaass']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17950ac7",
   "metadata": {},
   "source": [
    "Step 3: Phrase Modeling: Bigram and Trigram Models\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: 'back_bumper', 'oil_leakage', 'maryland_college_park' etc.\n",
    "\n",
    "Gensim's Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold.\n",
    "\n",
    "The higher the values of these param, the harder it is for words to be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e6d9cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c2a89c",
   "metadata": {},
   "source": [
    "Remove Stopwords, Make Bigrams and Lemmatize\n",
    "The phrase models are ready. Let’s define the functions to remove the stopwords, make trigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b568d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/damlasenturk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbbd8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b17c07ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/damlasenturk/miniconda3/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0331016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simplify', 'rule', 'theoretical', 'analysis', 'information_bottleneck', 'optimization', 'pca', 'spiking_neuron', 'lar', 'buese', 'technology', 'abstract', 'show', 'suitable', 'assumption', 'primarily', 'linearization', 'simple', 'perspicuous', 'online', 'learning', 'rule', 'optimization', 'spiking_neuron', 'derive', 'rule', 'perform', 'common', 'benchmark', 'task']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bc90a",
   "metadata": {},
   "source": [
    "Step 4: Data transformation: Corpus and Dictionary\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebeb4715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 1), (3, 4), (4, 2), (5, 1), (6, 2), (7, 1), (8, 1), (9, 4), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 10), (18, 1), (19, 1), (20, 1), (21, 9), (22, 3), (23, 2), (24, 4), (25, 1), (26, 1), (27, 5), (28, 2), (29, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1b4d0",
   "metadata": {},
   "source": [
    "Step 5: Base Model\n",
    "We have everything required to train the base LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well. Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior (we'll use default for the base model).\n",
    "\n",
    "chunksize controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.\n",
    "\n",
    "passes controls how often we train the model on the entire corpus (set to 10). Another word for passes might be \"epochs\". iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \"passes\" and \"iterations\" high enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e83677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f964605",
   "metadata": {},
   "source": [
    "The above LDA model is built with 10 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efd4724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.025*\"model\" + 0.012*\"use\" + 0.011*\"network\" + 0.010*\"graph\" + 0.010*\"set\" '\n",
      "  '+ 0.008*\"learn\" + 0.007*\"give\" + 0.007*\"structure\" + 0.007*\"datum\" + '\n",
      "  '0.007*\"distribution\"'),\n",
      " (1,\n",
      "  '0.023*\"model\" + 0.010*\"influence\" + 0.010*\"use\" + 0.009*\"graph\" + '\n",
      "  '0.009*\"parameter\" + 0.009*\"function\" + 0.008*\"set\" + 0.008*\"result\" + '\n",
      "  '0.008*\"cluster\" + 0.008*\"time\"'),\n",
      " (2,\n",
      "  '0.009*\"method\" + 0.009*\"feature\" + 0.008*\"set\" + 0.008*\"training\" + '\n",
      "  '0.008*\"use\" + 0.007*\"figure\" + 0.007*\"system\" + 0.006*\"neural\" + '\n",
      "  '0.006*\"learn\" + 0.006*\"rate\"'),\n",
      " (3,\n",
      "  '0.011*\"network\" + 0.010*\"cluster\" + 0.010*\"game\" + 0.009*\"player\" + '\n",
      "  '0.008*\"noise\" + 0.008*\"point\" + 0.008*\"value\" + 0.008*\"set\" + 0.007*\"use\" + '\n",
      "  '0.006*\"learn\"'),\n",
      " (4,\n",
      "  '0.008*\"model\" + 0.008*\"use\" + 0.008*\"point\" + 0.008*\"datum\" + '\n",
      "  '0.007*\"result\" + 0.007*\"problem\" + 0.007*\"cluster\" + 0.007*\"order\" + '\n",
      "  '0.006*\"error\" + 0.006*\"number\"'),\n",
      " (5,\n",
      "  '0.010*\"use\" + 0.010*\"problem\" + 0.010*\"function\" + 0.009*\"example\" + '\n",
      "  '0.009*\"learn\" + 0.008*\"set\" + 0.008*\"model\" + 0.008*\"method\" + '\n",
      "  '0.007*\"optimal\" + 0.006*\"solution\"'),\n",
      " (6,\n",
      "  '0.019*\"graph\" + 0.013*\"network\" + 0.013*\"time\" + 0.013*\"number\" + '\n",
      "  '0.012*\"set\" + 0.012*\"learn\" + 0.011*\"matrix\" + 0.010*\"edge\" + '\n",
      "  '0.009*\"bayesian\" + 0.008*\"kernel\"'),\n",
      " (7,\n",
      "  '0.011*\"model\" + 0.010*\"sample\" + 0.009*\"set\" + 0.009*\"use\" + '\n",
      "  '0.009*\"distribution\" + 0.007*\"network\" + 0.007*\"function\" + 0.006*\"learn\" + '\n",
      "  '0.006*\"parameter\" + 0.005*\"experiment\"'),\n",
      " (8,\n",
      "  '0.010*\"function\" + 0.007*\"input\" + 0.007*\"weight\" + 0.007*\"result\" + '\n",
      "  '0.006*\"set\" + 0.006*\"object\" + 0.006*\"use\" + 0.006*\"show\" + 0.006*\"neuron\" '\n",
      "  '+ 0.006*\"cell\"'),\n",
      " (9,\n",
      "  '0.014*\"model\" + 0.014*\"learn\" + 0.010*\"task\" + 0.010*\"use\" + 0.009*\"set\" + '\n",
      "  '0.009*\"method\" + 0.006*\"input\" + 0.006*\"datum\" + 0.006*\"sample\" + '\n",
      "  '0.006*\"distribution\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e4b7b",
   "metadata": {},
   "source": [
    "Compute Model Perplexity and Coherence Score\n",
    "Let's calculate the baseline coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "769cc5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.2715528592528609\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab04d45",
   "metadata": {},
   "source": [
    "Step 6: Hyperparameter tuning¶\n",
    "First, let's differentiate between model hyperparameters and model parameters :\n",
    "\n",
    "Model hyperparameters can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
    "\n",
    "Model parameters can be thought of as what the model learns during training, such as the weights for each word in a given topic.\n",
    "\n",
    "Now that we have the baseline coherence score for the default LDA model, let's perform a series of sensitivity tests to help determine the following model hyperparameters:\n",
    "\n",
    "Number of Topics (K)\n",
    "Dirichlet hyperparameter alpha: Document-Topic Density\n",
    "Dirichlet hyperparameter beta: Word-Topic Density\n",
    "We'll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two difference validation corpus sets. We'll use C_v as our choice of metric for performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2349a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|██▋                                     | 36/540 [17:13<4:01:11, 28.71s/it]\u001b[A\n",
      "\n",
      "  0%|                                         | 1/540 [00:12<1:54:18, 12.73s/it]\u001b[A\n",
      "  0%|▏                                        | 2/540 [00:24<1:46:35, 11.89s/it]\u001b[A\n",
      "  1%|▏                                        | 3/540 [00:35<1:46:34, 11.91s/it]\u001b[A\n",
      "  1%|▎                                        | 4/540 [00:47<1:45:24, 11.80s/it]\u001b[A\n",
      "  1%|▍                                        | 5/540 [00:59<1:44:16, 11.69s/it]\u001b[A\n",
      "  1%|▍                                        | 6/540 [01:10<1:43:37, 11.64s/it]\u001b[A\n",
      "  1%|▌                                        | 7/540 [01:22<1:44:38, 11.78s/it]\u001b[A\n",
      "  1%|▌                                        | 8/540 [01:34<1:44:11, 11.75s/it]\u001b[A\n",
      "  2%|▋                                        | 9/540 [01:46<1:43:54, 11.74s/it]\u001b[A\n",
      "  2%|▋                                       | 10/540 [01:57<1:43:03, 11.67s/it]\u001b[A\n",
      "  2%|▊                                       | 11/540 [02:09<1:42:48, 11.66s/it]\u001b[A\n",
      "  2%|▉                                       | 12/540 [02:20<1:42:07, 11.60s/it]\u001b[A\n",
      "  2%|▉                                       | 13/540 [02:31<1:40:52, 11.48s/it]\u001b[A\n",
      "  3%|█                                       | 14/540 [02:43<1:40:22, 11.45s/it]\u001b[A\n",
      "  3%|█                                       | 15/540 [02:54<1:39:56, 11.42s/it]\u001b[A\n",
      "  3%|█▏                                      | 16/540 [03:06<1:40:44, 11.53s/it]\u001b[A\n",
      "  3%|█▎                                      | 17/540 [03:18<1:41:13, 11.61s/it]\u001b[A\n",
      "  3%|█▎                                      | 18/540 [03:30<1:43:18, 11.87s/it]\u001b[A\n",
      "  4%|█▍                                      | 19/540 [03:42<1:42:06, 11.76s/it]\u001b[A\n",
      "  4%|█▍                                      | 20/540 [03:53<1:41:49, 11.75s/it]\u001b[A\n",
      "  4%|█▌                                      | 21/540 [04:05<1:41:07, 11.69s/it]\u001b[A\n",
      "  4%|█▋                                      | 22/540 [04:16<1:39:43, 11.55s/it]\u001b[A\n",
      "  4%|█▋                                      | 23/540 [04:28<1:38:59, 11.49s/it]\u001b[A\n",
      "  4%|█▊                                      | 24/540 [04:39<1:38:14, 11.42s/it]\u001b[A\n",
      "  5%|█▊                                      | 25/540 [04:51<1:39:25, 11.58s/it]\u001b[A\n",
      "  5%|█▉                                      | 26/540 [05:03<1:39:33, 11.62s/it]\u001b[A\n",
      "  5%|██                                      | 27/540 [05:14<1:40:02, 11.70s/it]\u001b[A\n",
      "  5%|██                                      | 28/540 [05:28<1:44:50, 12.29s/it]\u001b[A\n",
      "  5%|██▏                                     | 29/540 [05:40<1:44:43, 12.30s/it]\u001b[A\n",
      "  6%|██▏                                     | 30/540 [05:53<1:45:19, 12.39s/it]\u001b[A\n",
      "  6%|██▎                                     | 31/540 [06:05<1:44:05, 12.27s/it]\u001b[A\n",
      "  6%|██▎                                     | 32/540 [06:16<1:41:08, 11.95s/it]\u001b[A\n",
      "  6%|██▍                                     | 33/540 [06:28<1:40:03, 11.84s/it]\u001b[A\n",
      "  6%|██▌                                     | 34/540 [06:39<1:38:25, 11.67s/it]\u001b[A\n",
      "  6%|██▌                                     | 35/540 [06:50<1:36:45, 11.50s/it]\u001b[A\n",
      "  7%|██▋                                     | 36/540 [07:01<1:35:46, 11.40s/it]\u001b[A\n",
      "  7%|██▋                                     | 37/540 [07:12<1:34:57, 11.33s/it]\u001b[A\n",
      "  7%|██▊                                     | 38/540 [07:24<1:35:44, 11.44s/it]\u001b[A\n",
      "  7%|██▉                                     | 39/540 [07:36<1:36:50, 11.60s/it]\u001b[A\n",
      "  7%|██▉                                     | 40/540 [07:47<1:35:36, 11.47s/it]\u001b[A\n",
      "  8%|███                                     | 41/540 [07:59<1:34:54, 11.41s/it]\u001b[A\n",
      "  8%|███                                     | 42/540 [08:11<1:36:38, 11.64s/it]\u001b[A\n",
      "  8%|███▏                                    | 43/540 [08:22<1:36:04, 11.60s/it]\u001b[A\n",
      "  8%|███▎                                    | 44/540 [08:34<1:35:09, 11.51s/it]\u001b[A\n",
      "  8%|███▎                                    | 45/540 [08:45<1:34:03, 11.40s/it]\u001b[A\n",
      "  9%|███▍                                    | 46/540 [08:56<1:33:07, 11.31s/it]\u001b[A\n",
      "  9%|███▍                                    | 47/540 [09:07<1:33:30, 11.38s/it]\u001b[A\n",
      "  9%|███▌                                    | 48/540 [09:19<1:33:13, 11.37s/it]\u001b[A\n",
      "  9%|███▋                                    | 49/540 [09:30<1:33:12, 11.39s/it]\u001b[A\n",
      "  9%|███▋                                    | 50/540 [09:42<1:33:40, 11.47s/it]\u001b[A\n",
      "  9%|███▊                                    | 51/540 [09:53<1:33:54, 11.52s/it]\u001b[A\n",
      " 10%|███▊                                    | 52/540 [10:05<1:33:25, 11.49s/it]\u001b[A\n",
      " 10%|███▉                                    | 53/540 [10:16<1:32:56, 11.45s/it]\u001b[A\n",
      " 10%|████                                    | 54/540 [10:27<1:32:07, 11.37s/it]\u001b[A\n",
      " 10%|████                                    | 55/540 [10:38<1:30:58, 11.25s/it]\u001b[A\n",
      " 10%|████▏                                   | 56/540 [10:50<1:31:36, 11.36s/it]\u001b[A\n",
      " 11%|████▏                                   | 57/540 [11:01<1:31:29, 11.37s/it]\u001b[A\n",
      " 11%|████▎                                   | 58/540 [11:13<1:31:04, 11.34s/it]\u001b[A\n",
      " 11%|████▎                                   | 59/540 [11:24<1:30:52, 11.34s/it]\u001b[A\n",
      " 11%|████▍                                   | 60/540 [11:36<1:32:21, 11.54s/it]\u001b[A\n",
      " 11%|████▌                                   | 61/540 [11:48<1:32:18, 11.56s/it]\u001b[A\n",
      " 11%|████▌                                   | 62/540 [11:59<1:32:00, 11.55s/it]\u001b[A\n",
      " 12%|████▋                                   | 63/540 [12:11<1:31:29, 11.51s/it]\u001b[A\n",
      " 12%|████▋                                   | 64/540 [12:22<1:31:42, 11.56s/it]\u001b[A\n",
      " 12%|████▊                                   | 65/540 [12:34<1:31:12, 11.52s/it]\u001b[A\n",
      " 12%|████▉                                   | 66/540 [12:45<1:30:36, 11.47s/it]\u001b[A\n",
      " 12%|████▉                                   | 67/540 [12:56<1:29:50, 11.40s/it]\u001b[A\n",
      " 13%|█████                                   | 68/540 [13:08<1:29:29, 11.38s/it]\u001b[A\n",
      " 13%|█████                                   | 69/540 [13:19<1:30:10, 11.49s/it]\u001b[A\n",
      " 13%|█████▏                                  | 70/540 [13:31<1:30:17, 11.53s/it]\u001b[A\n",
      " 13%|█████▎                                  | 71/540 [13:43<1:31:04, 11.65s/it]\u001b[A\n",
      " 13%|█████▎                                  | 72/540 [13:55<1:30:55, 11.66s/it]\u001b[A\n",
      " 14%|█████▍                                  | 73/540 [14:06<1:30:46, 11.66s/it]\u001b[A\n",
      " 14%|█████▍                                  | 74/540 [14:18<1:30:05, 11.60s/it]\u001b[A\n",
      " 14%|█████▌                                  | 75/540 [14:29<1:29:40, 11.57s/it]\u001b[A\n",
      " 14%|█████▋                                  | 76/540 [14:41<1:29:21, 11.56s/it]\u001b[A\n",
      " 14%|█████▋                                  | 77/540 [14:52<1:28:50, 11.51s/it]\u001b[A\n",
      " 14%|█████▊                                  | 78/540 [15:04<1:28:29, 11.49s/it]\u001b[A\n",
      " 15%|█████▊                                  | 79/540 [15:15<1:28:07, 11.47s/it]\u001b[A\n",
      " 15%|█████▉                                  | 80/540 [15:27<1:28:26, 11.53s/it]\u001b[A\n",
      " 15%|██████                                  | 81/540 [15:38<1:28:11, 11.53s/it]\u001b[A\n",
      " 15%|██████                                  | 82/540 [15:51<1:30:07, 11.81s/it]\u001b[A\n",
      " 15%|██████▏                                 | 83/540 [16:02<1:29:39, 11.77s/it]\u001b[A\n",
      " 16%|██████▏                                 | 84/540 [16:14<1:28:31, 11.65s/it]\u001b[A\n",
      " 16%|██████▎                                 | 85/540 [16:25<1:27:43, 11.57s/it]\u001b[A\n",
      " 16%|██████▎                                 | 86/540 [16:37<1:27:39, 11.59s/it]\u001b[A\n",
      " 16%|██████▍                                 | 87/540 [16:48<1:27:20, 11.57s/it]\u001b[A\n",
      " 16%|██████▌                                 | 88/540 [17:00<1:26:54, 11.54s/it]\u001b[A\n",
      " 16%|██████▌                                 | 89/540 [17:11<1:26:10, 11.46s/it]\u001b[A\n",
      " 17%|██████▋                                 | 90/540 [17:22<1:25:52, 11.45s/it]\u001b[A\n",
      " 17%|██████▋                                 | 91/540 [17:34<1:26:47, 11.60s/it]\u001b[A\n",
      " 17%|██████▊                                 | 92/540 [17:47<1:27:56, 11.78s/it]\u001b[A\n",
      " 17%|██████▉                                 | 93/540 [17:58<1:27:40, 11.77s/it]\u001b[A\n",
      " 17%|██████▉                                 | 94/540 [18:10<1:27:11, 11.73s/it]\u001b[A\n",
      " 18%|███████                                 | 95/540 [18:22<1:27:48, 11.84s/it]\u001b[A\n",
      " 18%|███████                                 | 96/540 [18:34<1:27:07, 11.77s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▏                                | 97/540 [18:45<1:26:35, 11.73s/it]\u001b[A\n",
      " 18%|███████▎                                | 98/540 [18:57<1:26:24, 11.73s/it]\u001b[A\n",
      " 18%|███████▎                                | 99/540 [19:09<1:26:41, 11.79s/it]\u001b[A\n",
      " 19%|███████▏                               | 100/540 [19:21<1:26:04, 11.74s/it]\u001b[A\n",
      " 19%|███████▎                               | 101/540 [19:32<1:25:47, 11.73s/it]\u001b[A\n",
      " 19%|███████▎                               | 102/540 [19:44<1:25:11, 11.67s/it]\u001b[A\n",
      " 19%|███████▍                               | 103/540 [19:56<1:27:09, 11.97s/it]\u001b[A\n",
      " 19%|███████▌                               | 104/540 [20:08<1:25:56, 11.83s/it]\u001b[A\n",
      " 19%|███████▌                               | 105/540 [20:19<1:24:44, 11.69s/it]\u001b[A\n",
      " 20%|███████▋                               | 106/540 [20:31<1:24:21, 11.66s/it]\u001b[A\n",
      " 20%|███████▋                               | 107/540 [20:42<1:23:45, 11.61s/it]\u001b[A\n",
      " 20%|███████▊                               | 108/540 [20:54<1:24:28, 11.73s/it]\u001b[A\n",
      " 20%|███████▊                               | 109/540 [21:06<1:24:13, 11.73s/it]\u001b[A\n",
      " 20%|███████▉                               | 110/540 [21:18<1:24:24, 11.78s/it]\u001b[A\n",
      " 21%|████████                               | 111/540 [21:29<1:23:15, 11.64s/it]\u001b[A\n",
      " 21%|████████                               | 112/540 [21:41<1:23:12, 11.67s/it]\u001b[A\n",
      " 21%|████████▏                              | 113/540 [21:53<1:23:46, 11.77s/it]\u001b[A\n",
      " 21%|████████▏                              | 114/540 [22:05<1:23:18, 11.73s/it]\u001b[A\n",
      " 21%|████████▎                              | 115/540 [22:16<1:22:22, 11.63s/it]\u001b[A\n",
      " 21%|████████▍                              | 116/540 [22:28<1:22:49, 11.72s/it]\u001b[A\n",
      " 22%|████████▍                              | 117/540 [22:40<1:22:23, 11.69s/it]\u001b[A\n",
      " 22%|████████▌                              | 118/540 [22:51<1:21:29, 11.59s/it]\u001b[A\n",
      " 22%|████████▌                              | 119/540 [23:03<1:21:08, 11.56s/it]\u001b[A\n",
      " 22%|████████▋                              | 120/540 [23:14<1:20:55, 11.56s/it]\u001b[A\n",
      " 22%|████████▋                              | 121/540 [23:26<1:21:40, 11.69s/it]\u001b[A\n",
      " 23%|████████▊                              | 122/540 [23:38<1:21:33, 11.71s/it]\u001b[A\n",
      " 23%|████████▉                              | 123/540 [23:50<1:21:29, 11.73s/it]\u001b[A\n",
      " 23%|████████▉                              | 124/540 [24:01<1:21:16, 11.72s/it]\u001b[A\n",
      " 23%|█████████                              | 125/540 [24:13<1:21:16, 11.75s/it]\u001b[A\n",
      " 23%|█████████                              | 126/540 [24:25<1:20:43, 11.70s/it]\u001b[A\n",
      " 24%|█████████▏                             | 127/540 [24:36<1:20:30, 11.70s/it]\u001b[A\n",
      " 24%|█████████▏                             | 128/540 [24:48<1:20:26, 11.71s/it]\u001b[A\n",
      " 24%|█████████▎                             | 129/540 [25:00<1:20:17, 11.72s/it]\u001b[A\n",
      " 24%|█████████▍                             | 130/540 [25:12<1:20:00, 11.71s/it]\u001b[A\n",
      " 24%|█████████▍                             | 131/540 [25:23<1:19:55, 11.72s/it]\u001b[A\n",
      " 24%|█████████▌                             | 132/540 [25:35<1:20:14, 11.80s/it]\u001b[A\n",
      " 25%|█████████▌                             | 133/540 [25:47<1:19:46, 11.76s/it]\u001b[A\n",
      " 25%|█████████▋                             | 134/540 [25:59<1:20:10, 11.85s/it]\u001b[A\n",
      " 25%|█████████▊                             | 135/540 [26:11<1:19:57, 11.85s/it]\u001b[A\n",
      " 25%|█████████▊                             | 136/540 [26:23<1:20:04, 11.89s/it]\u001b[A\n",
      " 25%|█████████▉                             | 137/540 [26:35<1:19:48, 11.88s/it]\u001b[A\n",
      " 26%|█████████▉                             | 138/540 [26:47<1:19:45, 11.91s/it]\u001b[A\n",
      " 26%|██████████                             | 139/540 [26:58<1:18:51, 11.80s/it]\u001b[A\n",
      " 26%|██████████                             | 140/540 [27:10<1:18:30, 11.78s/it]\u001b[A\n",
      " 26%|██████████▏                            | 141/540 [27:22<1:18:03, 11.74s/it]\u001b[A\n",
      " 26%|██████████▎                            | 142/540 [27:34<1:18:21, 11.81s/it]\u001b[A\n",
      " 26%|██████████▎                            | 143/540 [27:46<1:18:43, 11.90s/it]\u001b[A\n",
      " 27%|██████████▍                            | 144/540 [27:58<1:18:47, 11.94s/it]\u001b[A\n",
      " 27%|██████████▍                            | 145/540 [28:10<1:18:22, 11.90s/it]\u001b[A\n",
      " 27%|██████████▌                            | 146/540 [28:21<1:17:46, 11.84s/it]\u001b[A\n",
      " 27%|██████████▌                            | 147/540 [28:33<1:17:29, 11.83s/it]\u001b[A\n",
      " 27%|██████████▋                            | 148/540 [28:45<1:17:16, 11.83s/it]\u001b[A\n",
      " 28%|██████████▊                            | 149/540 [28:56<1:16:42, 11.77s/it]\u001b[A\n",
      " 28%|██████████▊                            | 150/540 [29:08<1:16:13, 11.73s/it]\u001b[A\n",
      " 28%|██████████▉                            | 151/540 [29:20<1:16:44, 11.84s/it]\u001b[A\n",
      " 28%|██████████▉                            | 152/540 [29:32<1:16:34, 11.84s/it]\u001b[A\n",
      " 28%|███████████                            | 153/540 [29:44<1:16:09, 11.81s/it]\u001b[A\n",
      " 29%|███████████                            | 154/540 [29:55<1:15:45, 11.77s/it]\u001b[A\n",
      " 29%|███████████▏                           | 155/540 [30:08<1:16:51, 11.98s/it]\u001b[A\n",
      " 29%|███████████▎                           | 156/540 [30:20<1:16:31, 11.96s/it]\u001b[A\n",
      " 29%|███████████▎                           | 157/540 [30:32<1:15:59, 11.90s/it]\u001b[A\n",
      " 29%|███████████▍                           | 158/540 [30:43<1:15:34, 11.87s/it]\u001b[A\n",
      " 29%|███████████▍                           | 159/540 [30:56<1:15:57, 11.96s/it]\u001b[A\n",
      " 30%|███████████▌                           | 160/540 [31:08<1:16:01, 12.00s/it]\u001b[A\n",
      " 30%|███████████▋                           | 161/540 [31:20<1:15:36, 11.97s/it]\u001b[A\n",
      " 30%|███████████▋                           | 162/540 [31:31<1:15:12, 11.94s/it]\u001b[A\n",
      " 30%|███████████▊                           | 163/540 [31:43<1:15:07, 11.96s/it]\u001b[A\n",
      " 30%|███████████▊                           | 164/540 [31:55<1:14:28, 11.89s/it]\u001b[A\n",
      " 31%|███████████▉                           | 165/540 [32:07<1:14:09, 11.86s/it]\u001b[A\n",
      " 31%|███████████▉                           | 166/540 [32:19<1:14:04, 11.88s/it]\u001b[A\n",
      " 31%|████████████                           | 167/540 [32:31<1:13:46, 11.87s/it]\u001b[A\n",
      " 31%|████████████▏                          | 168/540 [32:43<1:14:13, 11.97s/it]\u001b[A\n",
      " 31%|████████████▏                          | 169/540 [32:55<1:13:55, 11.95s/it]\u001b[A\n",
      " 31%|████████████▎                          | 170/540 [33:07<1:13:24, 11.90s/it]\u001b[A\n",
      " 32%|████████████▎                          | 171/540 [33:19<1:13:13, 11.91s/it]\u001b[A\n",
      " 32%|████████████▍                          | 172/540 [33:31<1:13:05, 11.92s/it]\u001b[A\n",
      " 32%|████████████▍                          | 173/540 [33:42<1:12:20, 11.83s/it]\u001b[A\n",
      " 32%|████████████▌                          | 174/540 [33:54<1:11:59, 11.80s/it]\u001b[A\n",
      " 32%|████████████▋                          | 175/540 [34:06<1:12:48, 11.97s/it]\u001b[A\n",
      " 33%|████████████▋                          | 176/540 [34:19<1:13:17, 12.08s/it]\u001b[A\n",
      " 33%|████████████▊                          | 177/540 [34:31<1:12:51, 12.04s/it]\u001b[A\n",
      " 33%|████████████▊                          | 178/540 [34:42<1:12:26, 12.01s/it]\u001b[A\n",
      " 33%|████████████▉                          | 179/540 [34:54<1:12:00, 11.97s/it]\u001b[A\n",
      " 33%|█████████████                          | 180/540 [35:06<1:11:03, 11.84s/it]\u001b[A\n",
      " 34%|█████████████                          | 181/540 [35:18<1:10:56, 11.86s/it]\u001b[A\n",
      " 34%|█████████████▏                         | 182/540 [35:30<1:11:04, 11.91s/it]\u001b[A\n",
      " 34%|█████████████▏                         | 183/540 [35:42<1:11:10, 11.96s/it]\u001b[A\n",
      " 34%|█████████████▎                         | 184/540 [35:54<1:10:59, 11.96s/it]\u001b[A\n",
      " 34%|█████████████▎                         | 185/540 [36:06<1:11:19, 12.06s/it]\u001b[A\n",
      " 34%|█████████████▍                         | 186/540 [36:18<1:11:26, 12.11s/it]\u001b[A\n",
      " 35%|█████████████▌                         | 187/540 [36:30<1:10:48, 12.03s/it]\u001b[A\n",
      " 35%|█████████████▌                         | 188/540 [36:43<1:11:12, 12.14s/it]\u001b[A\n",
      " 35%|█████████████▋                         | 189/540 [36:55<1:11:47, 12.27s/it]\u001b[A\n",
      " 35%|█████████████▋                         | 190/540 [37:07<1:11:02, 12.18s/it]\u001b[A\n",
      " 35%|█████████████▊                         | 191/540 [37:19<1:10:39, 12.15s/it]\u001b[A\n",
      " 36%|█████████████▊                         | 192/540 [37:31<1:10:06, 12.09s/it]\u001b[A\n",
      " 36%|█████████████▉                         | 193/540 [37:44<1:11:35, 12.38s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████                         | 194/540 [37:57<1:12:37, 12.59s/it]\u001b[A\n",
      " 36%|██████████████                         | 195/540 [38:11<1:14:38, 12.98s/it]\u001b[A\n",
      " 36%|██████████████▏                        | 196/540 [38:25<1:15:48, 13.22s/it]\u001b[A\n",
      " 36%|██████████████▏                        | 197/540 [38:38<1:15:26, 13.20s/it]\u001b[A\n",
      " 37%|██████████████▎                        | 198/540 [38:51<1:14:47, 13.12s/it]\u001b[A\n",
      " 37%|██████████████▎                        | 199/540 [39:04<1:14:08, 13.05s/it]\u001b[A\n",
      " 37%|██████████████▍                        | 200/540 [39:17<1:13:25, 12.96s/it]\u001b[A\n",
      " 37%|██████████████▌                        | 201/540 [39:30<1:13:11, 12.96s/it]\u001b[A\n",
      " 37%|██████████████▌                        | 202/540 [39:42<1:12:25, 12.86s/it]\u001b[A\n",
      " 38%|██████████████▋                        | 203/540 [39:55<1:11:43, 12.77s/it]\u001b[A\n",
      " 38%|██████████████▋                        | 204/540 [40:08<1:11:25, 12.76s/it]\u001b[A\n",
      " 38%|██████████████▊                        | 205/540 [40:21<1:11:33, 12.82s/it]\u001b[A\n",
      " 38%|██████████████▉                        | 206/540 [40:33<1:10:38, 12.69s/it]\u001b[A\n",
      " 38%|██████████████▉                        | 207/540 [40:45<1:09:27, 12.51s/it]\u001b[A\n",
      " 39%|███████████████                        | 208/540 [40:57<1:08:15, 12.34s/it]\u001b[A\n",
      " 39%|███████████████                        | 209/540 [41:09<1:07:51, 12.30s/it]\u001b[A\n",
      " 39%|███████████████▏                       | 210/540 [41:21<1:07:22, 12.25s/it]\u001b[A\n",
      " 39%|███████████████▏                       | 211/540 [41:33<1:06:36, 12.15s/it]\u001b[A\n",
      " 39%|███████████████▎                       | 212/540 [41:45<1:05:50, 12.04s/it]\u001b[A\n",
      " 39%|███████████████▍                       | 213/540 [41:57<1:05:39, 12.05s/it]\u001b[A\n",
      " 40%|███████████████▍                       | 214/540 [42:09<1:05:21, 12.03s/it]\u001b[A\n",
      " 40%|███████████████▌                       | 215/540 [42:21<1:05:13, 12.04s/it]\u001b[A\n",
      " 40%|███████████████▌                       | 216/540 [42:33<1:05:30, 12.13s/it]\u001b[A\n",
      " 40%|███████████████▋                       | 217/540 [42:46<1:05:14, 12.12s/it]\u001b[A\n",
      " 40%|███████████████▋                       | 218/540 [42:58<1:05:53, 12.28s/it]\u001b[A\n",
      " 41%|███████████████▊                       | 219/540 [43:11<1:05:58, 12.33s/it]\u001b[A\n",
      " 41%|███████████████▉                       | 220/540 [43:23<1:05:23, 12.26s/it]\u001b[A\n",
      " 41%|███████████████▉                       | 221/540 [43:35<1:05:00, 12.23s/it]\u001b[A\n",
      " 41%|████████████████                       | 222/540 [43:48<1:06:11, 12.49s/it]\u001b[A\n",
      " 41%|████████████████                       | 223/540 [44:01<1:06:54, 12.66s/it]\u001b[A\n",
      " 41%|████████████████▏                      | 224/540 [44:14<1:07:39, 12.85s/it]\u001b[A\n",
      " 42%|████████████████▎                      | 225/540 [44:28<1:09:30, 13.24s/it]\u001b[A\n",
      " 42%|████████████████▎                      | 226/540 [44:42<1:09:39, 13.31s/it]\u001b[A\n",
      " 42%|████████████████▍                      | 227/540 [44:55<1:09:18, 13.29s/it]\u001b[A\n",
      " 42%|████████████████▍                      | 228/540 [45:09<1:09:27, 13.36s/it]\u001b[A\n",
      " 42%|████████████████▌                      | 229/540 [45:22<1:09:10, 13.35s/it]\u001b[A\n",
      " 43%|████████████████▌                      | 230/540 [45:35<1:09:06, 13.38s/it]\u001b[A\n",
      " 43%|████████████████▋                      | 231/540 [45:49<1:08:24, 13.28s/it]\u001b[A\n",
      " 43%|████████████████▊                      | 232/540 [46:01<1:07:06, 13.07s/it]\u001b[A\n",
      " 43%|████████████████▊                      | 233/540 [46:14<1:06:23, 12.98s/it]\u001b[A\n",
      " 43%|████████████████▉                      | 234/540 [46:27<1:06:58, 13.13s/it]\u001b[A\n",
      " 44%|████████████████▉                      | 235/540 [46:40<1:05:26, 12.88s/it]\u001b[A\n",
      " 44%|█████████████████                      | 236/540 [46:52<1:04:06, 12.65s/it]\u001b[A\n",
      " 44%|█████████████████                      | 237/540 [47:04<1:03:25, 12.56s/it]\u001b[A\n",
      " 44%|█████████████████▏                     | 238/540 [47:17<1:03:03, 12.53s/it]\u001b[A\n",
      " 44%|█████████████████▎                     | 239/540 [47:29<1:02:36, 12.48s/it]\u001b[A\n",
      " 44%|█████████████████▎                     | 240/540 [47:41<1:02:07, 12.42s/it]\u001b[A\n",
      " 45%|█████████████████▍                     | 241/540 [47:54<1:01:48, 12.40s/it]\u001b[A\n",
      " 45%|█████████████████▍                     | 242/540 [48:07<1:02:24, 12.56s/it]\u001b[A\n",
      " 45%|█████████████████▌                     | 243/540 [48:21<1:04:16, 12.98s/it]\u001b[A\n",
      " 45%|█████████████████▌                     | 244/540 [48:35<1:05:58, 13.37s/it]\u001b[A\n",
      " 45%|█████████████████▋                     | 245/540 [48:48<1:05:04, 13.24s/it]\u001b[A\n",
      " 46%|█████████████████▊                     | 246/540 [49:02<1:06:08, 13.50s/it]\u001b[A\n",
      " 46%|█████████████████▊                     | 247/540 [49:15<1:05:55, 13.50s/it]\u001b[A\n",
      " 46%|█████████████████▉                     | 248/540 [49:28<1:03:56, 13.14s/it]\u001b[A\n",
      " 46%|█████████████████▉                     | 249/540 [49:40<1:02:51, 12.96s/it]\u001b[A\n",
      " 46%|██████████████████                     | 250/540 [49:53<1:03:00, 13.04s/it]\u001b[A\n",
      " 46%|██████████████████▏                    | 251/540 [50:06<1:02:01, 12.88s/it]\u001b[A\n",
      " 47%|██████████████████▏                    | 252/540 [50:19<1:01:49, 12.88s/it]\u001b[A\n",
      " 47%|██████████████████▎                    | 253/540 [50:32<1:01:36, 12.88s/it]\u001b[A\n",
      " 47%|██████████████████▎                    | 254/540 [50:45<1:02:16, 13.06s/it]\u001b[A\n",
      " 47%|██████████████████▍                    | 255/540 [50:58<1:01:44, 13.00s/it]\u001b[A\n",
      " 47%|██████████████████▍                    | 256/540 [51:11<1:00:57, 12.88s/it]\u001b[A\n",
      " 48%|██████████████████▌                    | 257/540 [51:24<1:01:21, 13.01s/it]\u001b[A\n",
      " 48%|██████████████████▋                    | 258/540 [51:37<1:00:59, 12.98s/it]\u001b[A\n",
      " 48%|██████████████████▋                    | 259/540 [51:50<1:00:37, 12.95s/it]\u001b[A\n",
      " 48%|███████████████████▋                     | 260/540 [52:02<59:57, 12.85s/it]\u001b[A\n",
      " 48%|███████████████████▊                     | 261/540 [52:15<59:43, 12.84s/it]\u001b[A\n",
      " 49%|███████████████████▉                     | 262/540 [52:28<59:26, 12.83s/it]\u001b[A\n",
      " 49%|██████████████████▉                    | 263/540 [52:41<1:00:05, 13.02s/it]\u001b[A\n",
      " 49%|████████████████████                     | 264/540 [52:54<59:32, 12.94s/it]\u001b[A\n",
      " 49%|████████████████████                     | 265/540 [53:07<58:50, 12.84s/it]\u001b[A\n",
      " 49%|████████████████████▏                    | 266/540 [53:20<58:39, 12.85s/it]\u001b[A\n",
      " 49%|████████████████████▎                    | 267/540 [53:32<58:03, 12.76s/it]\u001b[A\n",
      " 50%|████████████████████▎                    | 268/540 [53:45<57:30, 12.69s/it]\u001b[A\n",
      " 50%|████████████████████▍                    | 269/540 [53:58<57:33, 12.74s/it]\u001b[A\n",
      " 50%|████████████████████▌                    | 270/540 [54:10<57:22, 12.75s/it]\u001b[A\n",
      " 50%|████████████████████▌                    | 271/540 [54:22<56:17, 12.56s/it]\u001b[A\n",
      " 50%|████████████████████▋                    | 272/540 [54:34<54:59, 12.31s/it]\u001b[A\n",
      " 51%|████████████████████▋                    | 273/540 [54:47<55:35, 12.49s/it]\u001b[A\n",
      " 51%|████████████████████▊                    | 274/540 [54:59<55:16, 12.47s/it]\u001b[A\n",
      " 51%|████████████████████▉                    | 275/540 [55:12<54:51, 12.42s/it]\u001b[A\n",
      " 51%|████████████████████▉                    | 276/540 [55:24<54:54, 12.48s/it]\u001b[A\n",
      " 51%|█████████████████████                    | 277/540 [55:37<54:24, 12.41s/it]\u001b[A\n",
      " 51%|█████████████████████                    | 278/540 [55:49<54:00, 12.37s/it]\u001b[A\n",
      " 52%|█████████████████████▏                   | 279/540 [56:01<53:54, 12.39s/it]\u001b[A\n",
      " 52%|█████████████████████▎                   | 280/540 [56:14<53:53, 12.44s/it]\u001b[A\n",
      " 52%|█████████████████████▎                   | 281/540 [56:26<53:22, 12.36s/it]\u001b[A\n",
      " 52%|█████████████████████▍                   | 282/540 [56:39<53:36, 12.47s/it]\u001b[A\n",
      " 52%|█████████████████████▍                   | 283/540 [56:52<53:53, 12.58s/it]\u001b[A\n",
      " 53%|█████████████████████▌                   | 284/540 [57:04<53:11, 12.47s/it]\u001b[A\n",
      " 53%|█████████████████████▋                   | 285/540 [57:16<52:14, 12.29s/it]\u001b[A\n",
      " 53%|█████████████████████▋                   | 286/540 [57:28<51:51, 12.25s/it]\u001b[A\n",
      " 53%|█████████████████████▊                   | 287/540 [57:40<51:49, 12.29s/it]\u001b[A\n",
      " 53%|█████████████████████▊                   | 288/540 [57:53<51:44, 12.32s/it]\u001b[A\n",
      " 54%|█████████████████████▉                   | 289/540 [58:05<51:41, 12.36s/it]\u001b[A\n",
      " 54%|██████████████████████                   | 290/540 [58:17<51:04, 12.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████                   | 291/540 [58:30<51:08, 12.32s/it]\u001b[A\n",
      " 54%|██████████████████████▏                  | 292/540 [58:43<51:53, 12.55s/it]\u001b[A\n",
      " 54%|██████████████████████▏                  | 293/540 [58:55<51:06, 12.41s/it]\u001b[A\n",
      " 54%|██████████████████████▎                  | 294/540 [59:07<50:39, 12.36s/it]\u001b[A\n",
      " 55%|██████████████████████▍                  | 295/540 [59:20<50:39, 12.40s/it]\u001b[A\n",
      " 55%|██████████████████████▍                  | 296/540 [59:33<51:12, 12.59s/it]\u001b[A\n",
      " 55%|██████████████████████▌                  | 297/540 [59:45<50:46, 12.54s/it]\u001b[A\n",
      " 55%|██████████████████████▋                  | 298/540 [59:57<50:02, 12.41s/it]\u001b[A\n",
      " 55%|█████████████████████▌                 | 299/540 [1:00:10<50:35, 12.59s/it]\u001b[A\n",
      " 56%|█████████████████████▋                 | 300/540 [1:00:22<49:47, 12.45s/it]\u001b[A\n",
      " 56%|█████████████████████▋                 | 301/540 [1:00:35<50:08, 12.59s/it]\u001b[A\n",
      " 56%|█████████████████████▊                 | 302/540 [1:00:49<51:24, 12.96s/it]\u001b[A\n",
      " 56%|█████████████████████▉                 | 303/540 [1:01:02<51:06, 12.94s/it]\u001b[A\n",
      " 56%|█████████████████████▉                 | 304/540 [1:01:15<51:31, 13.10s/it]\u001b[A\n",
      " 56%|██████████████████████                 | 305/540 [1:01:28<50:53, 12.99s/it]\u001b[A\n",
      " 57%|██████████████████████                 | 306/540 [1:01:41<50:10, 12.87s/it]\u001b[A\n",
      " 57%|██████████████████████▏                | 307/540 [1:01:53<49:47, 12.82s/it]\u001b[A\n",
      " 57%|██████████████████████▏                | 308/540 [1:02:06<49:11, 12.72s/it]\u001b[A\n",
      " 57%|██████████████████████▎                | 309/540 [1:02:18<48:29, 12.59s/it]\u001b[A\n",
      " 57%|██████████████████████▍                | 310/540 [1:02:31<48:13, 12.58s/it]\u001b[A\n",
      " 58%|██████████████████████▍                | 311/540 [1:02:44<48:17, 12.65s/it]\u001b[A\n",
      " 58%|██████████████████████▌                | 312/540 [1:02:57<48:55, 12.88s/it]\u001b[A\n",
      " 58%|██████████████████████▌                | 313/540 [1:03:09<47:57, 12.68s/it]\u001b[A\n",
      " 58%|██████████████████████▋                | 314/540 [1:03:22<47:34, 12.63s/it]\u001b[A\n",
      " 58%|██████████████████████▊                | 315/540 [1:03:35<48:03, 12.81s/it]\u001b[A\n",
      " 59%|██████████████████████▊                | 316/540 [1:03:48<48:03, 12.87s/it]\u001b[A\n",
      " 59%|██████████████████████▉                | 317/540 [1:04:01<47:50, 12.87s/it]\u001b[A\n",
      " 59%|██████████████████████▉                | 318/540 [1:04:14<47:39, 12.88s/it]\u001b[A\n",
      " 59%|███████████████████████                | 319/540 [1:04:27<47:27, 12.89s/it]\u001b[A\n",
      " 59%|███████████████████████                | 320/540 [1:04:39<46:21, 12.64s/it]\u001b[A\n",
      " 59%|███████████████████████▏               | 321/540 [1:04:51<46:07, 12.64s/it]\u001b[A\n",
      " 60%|███████████████████████▎               | 322/540 [1:05:04<46:01, 12.67s/it]\u001b[A\n",
      " 60%|███████████████████████▎               | 323/540 [1:05:16<45:16, 12.52s/it]\u001b[A\n",
      " 60%|███████████████████████▍               | 324/540 [1:05:28<44:36, 12.39s/it]\u001b[A\n",
      " 60%|███████████████████████▍               | 325/540 [1:05:40<44:04, 12.30s/it]\u001b[A\n",
      " 60%|███████████████████████▌               | 326/540 [1:05:52<43:36, 12.22s/it]\u001b[A\n",
      " 61%|███████████████████████▌               | 327/540 [1:06:05<44:01, 12.40s/it]\u001b[A\n",
      " 61%|███████████████████████▋               | 328/540 [1:06:18<43:56, 12.44s/it]\u001b[A\n",
      " 61%|███████████████████████▊               | 329/540 [1:06:30<44:01, 12.52s/it]\u001b[A\n",
      " 61%|███████████████████████▊               | 330/540 [1:06:43<43:53, 12.54s/it]\u001b[A\n",
      " 61%|███████████████████████▉               | 331/540 [1:06:56<43:43, 12.55s/it]\u001b[A\n",
      " 61%|███████████████████████▉               | 332/540 [1:07:08<43:42, 12.61s/it]\u001b[A\n",
      " 62%|████████████████████████               | 333/540 [1:07:21<43:02, 12.47s/it]\u001b[A\n",
      " 62%|████████████████████████               | 334/540 [1:07:33<42:33, 12.40s/it]\u001b[A\n",
      " 62%|████████████████████████▏              | 335/540 [1:07:45<42:13, 12.36s/it]\u001b[A\n",
      " 62%|████████████████████████▎              | 336/540 [1:07:57<41:55, 12.33s/it]\u001b[A\n",
      " 62%|████████████████████████▎              | 337/540 [1:08:09<41:23, 12.24s/it]\u001b[A\n",
      " 63%|████████████████████████▍              | 338/540 [1:08:21<40:57, 12.16s/it]\u001b[A\n",
      " 63%|████████████████████████▍              | 339/540 [1:08:33<40:35, 12.12s/it]\u001b[A\n",
      " 63%|████████████████████████▌              | 340/540 [1:08:46<40:57, 12.29s/it]\u001b[A\n",
      " 63%|████████████████████████▋              | 341/540 [1:08:59<41:53, 12.63s/it]\u001b[A\n",
      " 63%|████████████████████████▋              | 342/540 [1:09:12<41:50, 12.68s/it]\u001b[A\n",
      " 64%|████████████████████████▊              | 343/540 [1:09:24<41:12, 12.55s/it]\u001b[A\n",
      " 64%|████████████████████████▊              | 344/540 [1:09:37<40:57, 12.54s/it]\u001b[A\n",
      " 64%|████████████████████████▉              | 345/540 [1:09:50<41:27, 12.76s/it]\u001b[A\n",
      " 64%|████████████████████████▉              | 346/540 [1:10:03<41:34, 12.86s/it]\u001b[A\n",
      " 64%|█████████████████████████              | 347/540 [1:10:16<41:12, 12.81s/it]\u001b[A\n",
      " 64%|█████████████████████████▏             | 348/540 [1:10:29<41:17, 12.91s/it]\u001b[A\n",
      " 65%|█████████████████████████▏             | 349/540 [1:10:42<40:37, 12.76s/it]\u001b[A\n",
      " 65%|█████████████████████████▎             | 350/540 [1:10:54<40:08, 12.68s/it]\u001b[A\n",
      " 65%|█████████████████████████▎             | 351/540 [1:11:07<40:14, 12.77s/it]\u001b[A\n",
      " 65%|█████████████████████████▍             | 352/540 [1:11:20<39:44, 12.68s/it]\u001b[A\n",
      " 65%|█████████████████████████▍             | 353/540 [1:11:32<39:07, 12.56s/it]\u001b[A\n",
      " 66%|█████████████████████████▌             | 354/540 [1:11:44<38:41, 12.48s/it]\u001b[A\n",
      " 66%|█████████████████████████▋             | 355/540 [1:11:56<38:12, 12.39s/it]\u001b[A\n",
      " 66%|█████████████████████████▋             | 356/540 [1:12:09<37:56, 12.37s/it]\u001b[A\n",
      " 66%|█████████████████████████▊             | 357/540 [1:12:21<37:27, 12.28s/it]\u001b[A\n",
      " 66%|█████████████████████████▊             | 358/540 [1:12:33<37:28, 12.35s/it]\u001b[A\n",
      " 66%|█████████████████████████▉             | 359/540 [1:12:45<37:03, 12.28s/it]\u001b[A\n",
      " 67%|██████████████████████████             | 360/540 [1:12:58<37:03, 12.35s/it]\u001b[A\n",
      " 67%|██████████████████████████             | 361/540 [1:13:11<37:08, 12.45s/it]\u001b[A\n",
      " 67%|██████████████████████████▏            | 362/540 [1:13:23<36:40, 12.36s/it]\u001b[A\n",
      " 67%|██████████████████████████▏            | 363/540 [1:13:35<36:13, 12.28s/it]\u001b[A\n",
      " 67%|██████████████████████████▎            | 364/540 [1:13:47<35:49, 12.21s/it]\u001b[A\n",
      " 68%|██████████████████████████▎            | 365/540 [1:13:59<35:53, 12.30s/it]\u001b[A\n",
      " 68%|██████████████████████████▍            | 366/540 [1:14:12<35:48, 12.35s/it]\u001b[A\n",
      " 68%|██████████████████████████▌            | 367/540 [1:14:24<35:33, 12.33s/it]\u001b[A\n",
      " 68%|██████████████████████████▌            | 368/540 [1:14:36<35:17, 12.31s/it]\u001b[A\n",
      " 68%|██████████████████████████▋            | 369/540 [1:14:49<35:19, 12.40s/it]\u001b[A\n",
      " 69%|██████████████████████████▋            | 370/540 [1:15:02<35:31, 12.54s/it]\u001b[A\n",
      " 69%|██████████████████████████▊            | 371/540 [1:15:14<35:22, 12.56s/it]\u001b[A\n",
      " 69%|██████████████████████████▊            | 372/540 [1:15:27<35:11, 12.57s/it]\u001b[A\n",
      " 69%|██████████████████████████▉            | 373/540 [1:15:40<35:20, 12.70s/it]\u001b[A\n",
      " 69%|███████████████████████████            | 374/540 [1:15:53<35:08, 12.70s/it]\u001b[A\n",
      " 69%|███████████████████████████            | 375/540 [1:16:05<34:33, 12.57s/it]\u001b[A\n",
      " 70%|███████████████████████████▏           | 376/540 [1:16:17<34:11, 12.51s/it]\u001b[A\n",
      " 70%|███████████████████████████▏           | 377/540 [1:16:30<34:13, 12.60s/it]\u001b[A\n",
      " 70%|███████████████████████████▎           | 378/540 [1:16:43<33:54, 12.56s/it]\u001b[A\n",
      " 70%|███████████████████████████▎           | 379/540 [1:16:55<33:45, 12.58s/it]\u001b[A\n",
      " 70%|███████████████████████████▍           | 380/540 [1:17:08<33:43, 12.65s/it]\u001b[A\n",
      " 71%|███████████████████████████▌           | 381/540 [1:17:21<33:34, 12.67s/it]\u001b[A\n",
      " 71%|███████████████████████████▌           | 382/540 [1:17:33<33:07, 12.58s/it]\u001b[A\n",
      " 71%|███████████████████████████▋           | 383/540 [1:17:46<32:48, 12.54s/it]\u001b[A\n",
      " 71%|███████████████████████████▋           | 384/540 [1:17:58<32:20, 12.44s/it]\u001b[A\n",
      " 71%|███████████████████████████▊           | 385/540 [1:18:11<32:31, 12.59s/it]\u001b[A\n",
      " 71%|███████████████████████████▉           | 386/540 [1:18:23<32:02, 12.48s/it]\u001b[A\n",
      " 72%|███████████████████████████▉           | 387/540 [1:18:35<31:33, 12.37s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████           | 388/540 [1:18:47<31:16, 12.34s/it]\u001b[A\n",
      " 72%|████████████████████████████           | 389/540 [1:19:00<31:26, 12.50s/it]\u001b[A\n",
      " 72%|████████████████████████████▏          | 390/540 [1:19:14<32:26, 12.98s/it]\u001b[A\n",
      " 72%|████████████████████████████▏          | 391/540 [1:19:27<32:01, 12.90s/it]\u001b[A\n",
      " 73%|████████████████████████████▎          | 392/540 [1:19:39<31:19, 12.70s/it]\u001b[A\n",
      " 73%|████████████████████████████▍          | 393/540 [1:19:52<31:26, 12.83s/it]\u001b[A\n",
      " 73%|████████████████████████████▍          | 394/540 [1:20:05<30:54, 12.71s/it]\u001b[A\n",
      " 73%|████████████████████████████▌          | 395/540 [1:20:17<30:21, 12.56s/it]\u001b[A\n",
      " 73%|████████████████████████████▌          | 396/540 [1:20:30<30:12, 12.59s/it]\u001b[A\n",
      " 74%|████████████████████████████▋          | 397/540 [1:20:43<30:11, 12.67s/it]\u001b[A\n",
      " 74%|████████████████████████████▋          | 398/540 [1:20:55<29:48, 12.60s/it]\u001b[A\n",
      " 74%|████████████████████████████▊          | 399/540 [1:21:07<29:32, 12.57s/it]\u001b[A\n",
      " 74%|████████████████████████████▉          | 400/540 [1:21:20<29:22, 12.59s/it]\u001b[A\n",
      " 74%|████████████████████████████▉          | 401/540 [1:21:33<29:14, 12.63s/it]\u001b[A\n",
      " 74%|█████████████████████████████          | 402/540 [1:21:46<29:17, 12.73s/it]\u001b[A\n",
      " 75%|█████████████████████████████          | 403/540 [1:21:59<29:31, 12.93s/it]\u001b[A\n",
      " 75%|█████████████████████████████▏         | 404/540 [1:22:12<29:16, 12.92s/it]\u001b[A\n",
      " 75%|█████████████████████████████▎         | 405/540 [1:22:25<29:23, 13.06s/it]\u001b[A\n",
      " 75%|█████████████████████████████▎         | 406/540 [1:22:38<28:54, 12.94s/it]\u001b[A\n",
      " 75%|█████████████████████████████▍         | 407/540 [1:22:50<28:17, 12.76s/it]\u001b[A\n",
      " 76%|█████████████████████████████▍         | 408/540 [1:23:03<27:58, 12.72s/it]\u001b[A\n",
      " 76%|█████████████████████████████▌         | 409/540 [1:23:18<29:12, 13.38s/it]\u001b[A\n",
      " 76%|█████████████████████████████▌         | 410/540 [1:23:31<28:45, 13.27s/it]\u001b[A\n",
      " 76%|█████████████████████████████▋         | 411/540 [1:23:45<28:47, 13.39s/it]\u001b[A\n",
      " 76%|█████████████████████████████▊         | 412/540 [1:23:58<28:28, 13.35s/it]\u001b[A\n",
      " 76%|█████████████████████████████▊         | 413/540 [1:24:12<28:37, 13.53s/it]\u001b[A\n",
      " 77%|█████████████████████████████▉         | 414/540 [1:24:25<27:58, 13.32s/it]\u001b[A\n",
      " 77%|█████████████████████████████▉         | 415/540 [1:24:37<27:17, 13.10s/it]\u001b[A\n",
      " 77%|██████████████████████████████         | 416/540 [1:24:51<27:17, 13.21s/it]\u001b[A\n",
      " 77%|██████████████████████████████         | 417/540 [1:25:04<27:17, 13.31s/it]\u001b[A\n",
      " 77%|██████████████████████████████▏        | 418/540 [1:25:18<26:59, 13.27s/it]\u001b[A\n",
      " 78%|██████████████████████████████▎        | 419/540 [1:25:31<26:36, 13.19s/it]\u001b[A\n",
      " 78%|██████████████████████████████▎        | 420/540 [1:25:44<26:35, 13.29s/it]\u001b[A\n",
      " 78%|██████████████████████████████▍        | 421/540 [1:25:57<26:19, 13.27s/it]\u001b[A\n",
      " 78%|██████████████████████████████▍        | 422/540 [1:26:10<25:40, 13.05s/it]\u001b[A\n",
      " 78%|██████████████████████████████▌        | 423/540 [1:26:22<25:06, 12.88s/it]\u001b[A\n",
      " 79%|██████████████████████████████▌        | 424/540 [1:26:35<24:37, 12.74s/it]\u001b[A\n",
      " 79%|██████████████████████████████▋        | 425/540 [1:26:47<24:16, 12.67s/it]\u001b[A\n",
      " 79%|██████████████████████████████▊        | 426/540 [1:27:00<24:16, 12.77s/it]\u001b[A\n",
      " 79%|██████████████████████████████▊        | 427/540 [1:27:14<24:26, 12.98s/it]\u001b[A\n",
      " 79%|██████████████████████████████▉        | 428/540 [1:27:28<25:09, 13.48s/it]\u001b[A\n",
      " 79%|██████████████████████████████▉        | 429/540 [1:27:42<24:53, 13.46s/it]\u001b[A\n",
      " 80%|███████████████████████████████        | 430/540 [1:27:55<24:28, 13.35s/it]\u001b[A\n",
      " 80%|███████████████████████████████▏       | 431/540 [1:28:08<24:08, 13.29s/it]\u001b[A\n",
      " 80%|███████████████████████████████▏       | 432/540 [1:28:21<23:55, 13.29s/it]\u001b[A\n",
      " 80%|███████████████████████████████▎       | 433/540 [1:28:35<24:00, 13.46s/it]\u001b[A\n",
      " 80%|███████████████████████████████▎       | 434/540 [1:28:49<23:49, 13.48s/it]\u001b[A\n",
      " 81%|███████████████████████████████▍       | 435/540 [1:29:02<23:24, 13.37s/it]\u001b[A\n",
      " 81%|███████████████████████████████▍       | 436/540 [1:29:15<23:13, 13.40s/it]\u001b[A\n",
      " 81%|███████████████████████████████▌       | 437/540 [1:29:29<23:08, 13.48s/it]\u001b[A\n",
      " 81%|███████████████████████████████▋       | 438/540 [1:29:42<22:42, 13.35s/it]\u001b[A\n",
      " 81%|███████████████████████████████▋       | 439/540 [1:29:55<22:12, 13.19s/it]\u001b[A\n",
      " 81%|███████████████████████████████▊       | 440/540 [1:30:07<21:41, 13.02s/it]\u001b[A\n",
      " 82%|███████████████████████████████▊       | 441/540 [1:30:20<21:27, 13.01s/it]\u001b[A\n",
      " 82%|███████████████████████████████▉       | 442/540 [1:30:33<20:56, 12.82s/it]\u001b[A\n",
      " 82%|███████████████████████████████▉       | 443/540 [1:30:45<20:37, 12.76s/it]\u001b[A\n",
      " 82%|████████████████████████████████       | 444/540 [1:30:58<20:23, 12.75s/it]\u001b[A\n",
      " 82%|████████████████████████████████▏      | 445/540 [1:31:12<20:32, 12.97s/it]\u001b[A\n",
      " 83%|████████████████████████████████▏      | 446/540 [1:31:24<20:16, 12.94s/it]\u001b[A\n",
      " 83%|████████████████████████████████▎      | 447/540 [1:31:37<20:00, 12.91s/it]\u001b[A\n",
      " 83%|████████████████████████████████▎      | 448/540 [1:31:50<19:43, 12.86s/it]\u001b[A\n",
      " 83%|████████████████████████████████▍      | 449/540 [1:32:03<19:32, 12.88s/it]\u001b[A\n",
      " 83%|████████████████████████████████▌      | 450/540 [1:32:16<19:18, 12.87s/it]\u001b[A\n",
      " 84%|████████████████████████████████▌      | 451/540 [1:32:29<19:10, 12.92s/it]\u001b[A\n",
      " 84%|████████████████████████████████▋      | 452/540 [1:32:42<18:54, 12.89s/it]\u001b[A\n",
      " 84%|████████████████████████████████▋      | 453/540 [1:32:55<18:46, 12.95s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb339fc0",
   "metadata": {},
   "source": [
    "Step 7: Final Model\n",
    "Based on external evaluation (Code to be added from Excel based analysis), let's train the final model with parameters yielding highest coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdccbdc",
   "metadata": {},
   "source": [
    "Step 8: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668afeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca725a75",
   "metadata": {},
   "source": [
    "Hierarchical LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming documents is a list of tokenized documents\n",
    "dictionary = Dictionary(data_lemmatized)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Training the top-level LDA model\n",
    "top_model = LdaModel(corpus, num_topics=10, id2word=dictionary)\n",
    "\n",
    "# For each topic, we collect the documents most relevant to that topic\n",
    "topic_docs = {}\n",
    "for i, doc in enumerate(corpus):\n",
    "    topic = max(top_model[doc], key=lambda x: x[1])[0]\n",
    "    if topic not in topic_docs:\n",
    "        topic_docs[topic] = []\n",
    "    topic_docs[topic].append(documents[i])\n",
    "\n",
    "# Now we train a separate LDA model for each set of documents\n",
    "sub_models = {}\n",
    "for topic, docs in topic_docs.items():\n",
    "    sub_dictionary = Dictionary(docs)\n",
    "    sub_corpus = [sub_dictionary.doc2bow(doc) for doc in docs]\n",
    "    sub_model = LdaModel(sub_corpus, num_topics=5, id2word=sub_dictionary)\n",
    "    sub_models[topic] = sub_model\n",
    "    \n",
    "    # Visualize the topics\n",
    "    vis = gensimvis.prepare(sub_model, sub_corpus, sub_dictionary)\n",
    "    pyLDAvis.display(vis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93722432",
   "metadata": {},
   "source": [
    "Correlated Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51780784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the CTM\n",
    "ctm = CTM(input_vectorizer=\"tfidf\", n_components=10, model_type=\"prodLDA\")\n",
    "\n",
    "# fit the CTM to the data\n",
    "ctm.fit(data_lemmatized)\n",
    "\n",
    "# visualize\n",
    "topics = ctm.get_topic_lists(5)  # get top 5 words for each topic\n",
    "topic_word_distributions = ctm.get_topic_word_matrix()\n",
    "vis_data = gensimvis.prepare(topic_word_distributions, df['processed'], ctm.vectorizer)\n",
    "pyLDAvis.display(vis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
